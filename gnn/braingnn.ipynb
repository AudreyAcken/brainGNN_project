{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10266020,"sourceType":"datasetVersion","datasetId":6351175}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nGRAPH_DIR = \"/kaggle/input/graphs/pyg_graphs/\"\npt_files = sorted(glob.glob(os.path.join(GRAPH_DIR, \"*_kmeans*.pt\")))\n\nprint(f\"Found {len(pt_files)} graph files in {GRAPH_DIR}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graph_info_list = []\n\ntask_label_map = {\"WM\": 0, \"EMOTION\": 1, \"GAMBLING\":2, \"MOTOR\":3, \"RELATIONAL\":4, \"SOCIAL\":5, \"LANGUAGE\":6}\n\n# [\"WM\", \"EMOTION\", 'GAMBLING', 'MOTOR', 'RELATIONAL', 'SOCIAL', 'LANGUAGE'\nfor pt_path in pt_files:\n    fname = os.path.basename(pt_path)\n    parts = fname.split(\"_\") \n    \n    if len(parts) < 3:\n        continue  \n    \n    subject_id = parts[0]  \n    task_str = parts[2]  \n    \n    if task_str not in task_label_map:\n        # skip others for now\n        continue\n    \n    data = torch.load(pt_path)  \n\n    edge_index = data.edge_index.numpy()\n    edge_weight = data.edge_attr.numpy().flatten() if data.edge_attr is not None else np.ones(edge_index.shape[1])\n    \n    G_nx = nx.Graph()\n    for i in range(edge_index.shape[1]):\n        u = edge_index[0, i]\n        v = edge_index[1, i]\n        w = float(edge_weight[i])\n        if not G_nx.has_node(u):\n            G_nx.add_node(u)\n        if not G_nx.has_node(v):\n            G_nx.add_node(v)\n        G_nx.add_edge(u, v, weight=w)\n\n    n_nodes = G_nx.number_of_nodes()\n    n_edges = G_nx.number_of_edges()\n    density = nx.density(G_nx)\n    \n    weights = [d['weight'] for (_,_, d) in G_nx.edges(data=True)]\n    avg_weight = np.mean(weights) if len(weights) > 0 else 0\n    \n    strength_list = []\n    for node in G_nx.nodes():\n        s = sum(d['weight'] for (_,_,d) in G_nx.edges(node, data=True))\n        strength_list.append(s)\n    avg_strength = np.mean(strength_list) if len(strength_list) > 0 else 0\n\n    label = task_label_map[task_str]\n    graph_info = {\n        \"filepath\": pt_path,\n        \"subject\": subject_id,\n        \"task\": task_str,\n        \"label\": label,\n        \"n_nodes\": n_nodes,\n        \"n_edges\": n_edges,\n        \"density\": density,\n        \"avg_weight\": avg_weight,\n        \"avg_strength\": avg_strength,\n        \"pyg_data\": data,  # store the actual PyG graph\n    }\n    graph_info_list.append(graph_info)\n\nprint(f\"Loaded {len(graph_info_list)} graphs (WM/EMOTION).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(graph_info_list)\nprint(df.head())\n\ndf_box = df[['task','avg_weight']]\ndf_box.boxplot(by='task', column='avg_weight', grid=False)\nplt.title(\"Average edge weight by task\")\nplt.suptitle(\"\")\nplt.show()\n\ndf_box2 = df[['task','avg_strength']]\ndf_box2.boxplot(by='task', column='avg_strength', grid=False)\nplt.title(\"Average strength by task\")\nplt.suptitle(\"\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_graphs = []\nfor gi in graph_info_list:\n    data = gi[\"pyg_data\"]\n    data.y = torch.tensor([gi[\"label\"]], dtype=torch.long)\n    all_graphs.append(data)\n\nprint(f\"Number of labeled graphs for GNN: {len(all_graphs)}\")\n\ntrain_size = int(0.8 * len(all_graphs))\ntest_size = len(all_graphs) - train_size\n\nrng = np.random.default_rng(42)\nindices = np.arange(len(all_graphs))\nrng.shuffle(indices)\n\ntrain_indices = indices[:train_size]\ntest_indices = indices[train_size:]\n\ntrain_dataset = [all_graphs[i] for i in train_indices]\ntest_dataset = [all_graphs[i] for i in test_indices]\n\nprint(f\"Train set size: {len(train_dataset)}, Test set size: {len(test_dataset)}\")\n\nfrom torch_geometric.loader import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n\nclass GCNGraphClassifier(nn.Module):\n    def __init__(self, in_channels=1, hidden_dim=32, num_classes=2):\n        super().__init__()\n        # if data.x is shape [num_nodes, 1], in_channels=1\n        # GCN layers\n        self.conv1 = GCNConv(in_channels, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        # final linear\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        # x: [total_nodes_in_batch, in_channels]\n        # edge_index: [2, E]\n        # batch: [total_nodes_in_batch], indicates which graph each node belongs to\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n\n        x = global_mean_pool(x, batch)  # shape [num_graphs, hidden_dim]\n\n        # final linear\n        x = self.lin(x)  # shape [num_graphs, num_classes]\n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nsample_data = all_graphs[0]\nin_channels = sample_data.x.shape[1] if sample_data.x is not None else 1\nmodel = GCNGraphClassifier(in_channels=in_channels, hidden_dim=32, num_classes=2)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_epoch(loader):\n    model.train()\n    total_loss = 0\n    for batch_data in loader:\n        batch_data = batch_data.to(device)\n        optimizer.zero_grad()\n        out = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n        loss = criterion(out, batch_data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch_data.num_graphs\n    return total_loss / len(loader.dataset)\n\ndef eval_accuracy(loader):\n    model.eval()\n    correct = 0\n    total = 0\n    for batch_data in loader:\n        batch_data = batch_data.to(device)\n        out = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch_data.y).sum().item()\n        total += batch_data.num_graphs\n    return correct / total if total > 0 else 0\n\nepochs = 100\nfor epoch in range(1, epochs+1):\n    loss = train_epoch(train_loader)\n    acc_train = eval_accuracy(train_loader)\n    acc_test = eval_accuracy(test_loader)\n    print(f\"Epoch {epoch:02d}, Loss={loss:.4f}, TrainAcc={acc_train:.3f}, TestAcc={acc_test:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef data_to_adjacency(data: Data):\n    \"\"\"\n    Convert a PyG Data object into a (n_nodes x n_nodes) adjacency matrix (NumPy).\n    We'll use the edge weights from data.edge_attr if available.\n    \"\"\"\n    edge_index = data.edge_index.cpu().numpy()\n    if data.edge_attr is not None:\n        edge_weight = data.edge_attr.cpu().numpy().flatten()\n    else:\n        # If no edge_attr, assume weight=1\n        edge_weight = np.ones(edge_index.shape[1], dtype=np.float32)\n    \n    n_nodes = data.num_nodes\n    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n    \n    for i in range(edge_index.shape[1]):\n        u = edge_index[0, i]\n        v = edge_index[1, i]\n        w = edge_weight[i]\n        # Undirected => set both [u,v] and [v,u]\n        adj_matrix[u, v] = w\n        adj_matrix[v, u] = w\n    return adj_matrix\n\ntasks_of_interest = [\"WM\", \"EMOTION\", 'GAMBLING', 'MOTOR', 'RELATIONAL', 'SOCIAL', 'LANGUAGE']\nsample_size_per_task = 3\n\nplot_graphs = []\nfor tsk in tasks_of_interest:\n    subset = df[df[\"task\"] == tsk]\n    # pick a few\n    subset = subset.head(sample_size_per_task)\n    for idx, row in subset.iterrows():\n        plot_graphs.append((row[\"task\"], row[\"subject\"], row[\"pyg_data\"]))\n\nif len(plot_graphs) == 0:\n    print(\"No graphs found for the specified tasks.\")\nelse:\n    fig, axes = plt.subplots(len(plot_graphs), 1, figsize=(6, 4*len(plot_graphs)))\n    if len(plot_graphs) == 1:\n        axes = [axes]  # make it iterable\n    \n    for ax, (task_str, subj, gdata) in zip(axes, plot_graphs):\n        adj = data_to_adjacency(gdata)\n        im = ax.imshow(adj, cmap=\"bwr\", vmin=-1, vmax=1)  # assuming corr ranges -1..1\n        ax.set_title(f\"Subject: {subj}, Task: {task_str}, Nodes={gdata.num_nodes}\")\n        ax.set_xlabel(\"Node\")\n        ax.set_ylabel(\"Node\")\n        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6,4))\nsns.boxplot(data=df, x=\"task\", y=\"n_edges\")\nplt.title(\"Distribution of #Edges across Tasks\")\nplt.ylabel(\"#Edges\")\nplt.xlabel(\"Task\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.boxplot(data=df, x=\"task\", y=\"density\", color=\"lightgray\")\nsns.stripplot(data=df, x=\"task\", y=\"density\", color=\"red\", alpha=0.6)\nplt.title(\"Graph Density across Tasks\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Task\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nmetric = \"avg_strength\"\ndf_agg = df.groupby(\"task\")[metric].mean().reset_index()\n\nplt.figure(figsize=(6,4))\nsns.barplot(data=df_agg, x=\"task\", y=metric, color=\"skyblue\")\nplt.title(f\"Mean {metric} by Task\")\nplt.ylabel(f\"{metric}\")\nplt.xlabel(\"Task\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.violinplot(data=df, x=\"task\", y=\"avg_weight\")\nplt.title(\"Distribution of Avg Weight by Task\")\nplt.xlabel(\"Task\")\nplt.ylabel(\"Average Edge Weight\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}